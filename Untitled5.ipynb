{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94f2ba24-664c-4a35-bcf8-b2b20a7895bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded chunk: 92282\n",
      "Loaded chunk: 89586\n",
      "Loaded chunk: 89062\n",
      "Loaded chunk: 88925\n",
      "Loaded chunk: 89143\n",
      "Loaded chunk: 89040\n",
      "Loaded chunk: 89054\n",
      "Loaded chunk: 88978\n",
      "Loaded chunk: 89088\n",
      "Loaded chunk: 88880\n",
      "Loaded chunk: 88711\n",
      "Loaded chunk: 89058\n",
      "Loaded chunk: 88938\n",
      "Loaded chunk: 89047\n",
      "Loaded chunk: 88933\n",
      "Loaded chunk: 89036\n",
      "Loaded chunk: 88843\n",
      "Loaded chunk: 89163\n",
      "Loaded chunk: 89148\n",
      "Loaded chunk: 89015\n",
      "Loaded chunk: 89297\n",
      "Loaded chunk: 89008\n",
      "Loaded chunk: 89301\n",
      "Loaded chunk: 88634\n",
      "Loaded chunk: 88224\n",
      "Loaded chunk: 88727\n",
      "Loaded chunk: 88448\n",
      "Loaded chunk: 88473\n",
      "Loaded chunk: 88408\n",
      "Loaded chunk: 88428\n",
      "Loaded chunk: 89189\n",
      "Loaded chunk: 89175\n",
      "Loaded chunk: 89062\n",
      "Loaded chunk: 88879\n",
      "Loaded chunk: 88728\n",
      "Loaded chunk: 89116\n",
      "Loaded chunk: 88905\n",
      "Loaded chunk: 89028\n",
      "Loaded chunk: 88661\n",
      "Loaded chunk: 88736\n",
      "Loaded chunk: 88787\n",
      "Loaded chunk: 88561\n",
      "Loaded chunk: 88790\n",
      "Loaded chunk: 88386\n",
      "Loaded chunk: 88319\n",
      "Loaded chunk: 88437\n",
      "Loaded chunk: 87711\n",
      "Loaded chunk: 88492\n",
      "Loaded chunk: 88818\n",
      "Loaded chunk: 88865\n",
      "Loaded chunk: 88846\n",
      "Loaded chunk: 88643\n",
      "Loaded chunk: 88889\n",
      "Loaded chunk: 88677\n",
      "Loaded chunk: 88496\n",
      "Loaded chunk: 88752\n",
      "Loaded chunk: 88887\n",
      "Loaded chunk: 88726\n",
      "Loaded chunk: 88597\n",
      "Loaded chunk: 88623\n",
      "Loaded chunk: 88664\n",
      "Loaded chunk: 88745\n",
      "Loaded chunk: 88462\n",
      "Loaded chunk: 89107\n",
      "Loaded chunk: 88726\n",
      "Loaded chunk: 88967\n",
      "Loaded chunk: 88596\n",
      "Loaded chunk: 88814\n",
      "Loaded chunk: 88762\n",
      "Loaded chunk: 88596\n",
      "Loaded chunk: 88725\n",
      "Loaded chunk: 88711\n",
      "Loaded chunk: 88799\n",
      "Loaded chunk: 88694\n",
      "Loaded chunk: 88092\n",
      "Loaded chunk: 88497\n",
      "Loaded chunk: 88356\n",
      "Loaded chunk: 88616\n",
      "Loaded chunk: 88605\n",
      "Loaded chunk: 88345\n",
      "Loaded chunk: 88733\n",
      "Loaded chunk: 88717\n",
      "Loaded chunk: 88505\n",
      "Loaded chunk: 88857\n",
      "Loaded chunk: 88668\n",
      "Loaded chunk: 88789\n",
      "Loaded chunk: 88889\n",
      "Loaded chunk: 88646\n",
      "Loaded chunk: 88797\n",
      "Loaded chunk: 88857\n",
      "Loaded chunk: 88822\n",
      "Loaded chunk: 88920\n",
      "Loaded chunk: 88592\n",
      "Loaded chunk: 88853\n",
      "Loaded chunk: 88861\n",
      "Loaded chunk: 88698\n",
      "Loaded chunk: 88932\n",
      "Loaded chunk: 88807\n",
      "Loaded chunk: 88887\n",
      "Loaded chunk: 88868\n",
      "Loaded chunk: 88934\n",
      "Loaded chunk: 88869\n",
      "Loaded chunk: 89056\n",
      "Loaded chunk: 88672\n",
      "Loaded chunk: 88870\n",
      "Loaded chunk: 88839\n",
      "Loaded chunk: 89056\n",
      "Loaded chunk: 88976\n",
      "Loaded chunk: 88875\n",
      "Loaded chunk: 88831\n",
      "Loaded chunk: 88774\n",
      "Loaded chunk: 83453\n",
      "Loaded chunk: 67847\n",
      "Loaded chunk: 11207\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kolla\\AppData\\Local\\Temp\\ipykernel_20424\\1494159811.py:75: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  data = data.groupby('label', group_keys=False).apply(lambda x: x.sample(min(len(x), SAMPLE_SIZE//4), random_state=RANDOM_STATE))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training LogisticRegression...\n",
      "LogisticRegression Accuracy: 0.9873\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.99      0.98      2500\n",
      "         1.0       0.99      0.99      0.99      2500\n",
      "         2.0       0.99      0.99      0.99      2500\n",
      "         3.0       0.99      0.98      0.99      2500\n",
      "\n",
      "    accuracy                           0.99     10000\n",
      "   macro avg       0.99      0.99      0.99     10000\n",
      "weighted avg       0.99      0.99      0.99     10000\n",
      "\n",
      "\n",
      "Training MultinomialNB...\n",
      "MultinomialNB Accuracy: 0.9768\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.97      0.97      0.97      2500\n",
      "         1.0       0.98      0.97      0.98      2500\n",
      "         2.0       0.98      0.98      0.98      2500\n",
      "         3.0       0.98      0.98      0.98      2500\n",
      "\n",
      "    accuracy                           0.98     10000\n",
      "   macro avg       0.98      0.98      0.98     10000\n",
      "weighted avg       0.98      0.98      0.98     10000\n",
      "\n",
      "\n",
      "Training LinearSVC...\n",
      "LinearSVC Accuracy: 0.9908\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.99      0.99      2500\n",
      "         1.0       1.00      0.99      0.99      2500\n",
      "         2.0       0.99      1.00      1.00      2500\n",
      "         3.0       0.99      0.99      0.99      2500\n",
      "\n",
      "    accuracy                           0.99     10000\n",
      "   macro avg       0.99      0.99      0.99     10000\n",
      "weighted avg       0.99      0.99      0.99     10000\n",
      "\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\"\"\"\n",
    "Fixed version: indentation error corrected for clean_text() function\n",
    "Jupyter-ready script for Consumer Complaint text classification\n",
    "\"\"\"\n",
    "\n",
    "# %%\n",
    "import os, re, gc, joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')\n",
    "\n",
    "DATA_PATH = \"D:\\csv\\complaints.csv\"\n",
    "SAMPLE_SIZE = 50000\n",
    "RANDOM_STATE = 42\n",
    "LABEL_MAPPING = {\n",
    "    'Credit reporting': 0,\n",
    "    'Credit reporting, credit repair services, or other': 0,\n",
    "    'Debt collection': 1,\n",
    "    'Consumer Loan': 2,\n",
    "    'Mortgage': 3,\n",
    "}\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "contraction_mapping = {\"ain't\": \"is not\", \"can't\": \"cannot\", \"won't\": \"will not\", \"i'm\": \"i am\", \"it's\": \"it is\"}\n",
    "\n",
    "# fixed indentation\n",
    "def clean_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = text.lower()\n",
    "    for k, v in contraction_mapping.items():\n",
    "        text = text.replace(k, v)\n",
    "    text = re.sub(r'\\S+@\\S+', ' ', text)\n",
    "    text = re.sub(r'http\\S+|www\\.\\S+', ' ', text)\n",
    "    text = re.sub(r'[^a-z\\s]', ' ', text)\n",
    "    tokens = [w for w in text.split() if len(w) > 1 and w not in stop_words]\n",
    "    tokens = [lemmatizer.lemmatize(t) for t in tokens]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# %% Load data sample\n",
    "usecols = ['Product', 'Issue', 'Consumer complaint narrative']\n",
    "chunks = []\n",
    "for chunk in pd.read_csv(DATA_PATH, usecols=usecols, chunksize=100000):\n",
    "    def map_label(p):\n",
    "        if not isinstance(p, str):\n",
    "            return None\n",
    "        for k, v in LABEL_MAPPING.items():\n",
    "            if k.lower() in p.lower():\n",
    "                return v\n",
    "        return None\n",
    "    chunk['label'] = chunk['Product'].apply(map_label)\n",
    "    filtered = chunk[chunk['label'].notnull()]\n",
    "    chunks.append(filtered)\n",
    "    print('Loaded chunk:', len(filtered))\n",
    "\n",
    "data = pd.concat(chunks, ignore_index=True)\n",
    "data = data.groupby('label', group_keys=False).apply(lambda x: x.sample(min(len(x), SAMPLE_SIZE//4), random_state=RANDOM_STATE))\n",
    "data['text'] = data['Issue'].fillna('') + ' ' + data['Consumer complaint narrative'].fillna('')\n",
    "data['clean_text'] = data['text'].apply(clean_text)\n",
    "\n",
    "X = data['clean_text']\n",
    "y = data['label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=RANDOM_STATE)\n",
    "\n",
    "# %% Train models\n",
    "tfidf = TfidfVectorizer(max_features=20000, ngram_range=(1,2))\n",
    "models = {\n",
    "    'LogisticRegression': LogisticRegression(max_iter=1000, n_jobs=-1),\n",
    "    'MultinomialNB': MultinomialNB(),\n",
    "    'LinearSVC': LinearSVC(max_iter=5000)\n",
    "}\n",
    "\n",
    "for name, clf in models.items():\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    pipe = Pipeline([('tfidf', tfidf), ('clf', clf)])\n",
    "    pipe.fit(X_train, y_train)\n",
    "    preds = pipe.predict(X_test)\n",
    "    print(name, 'Accuracy:', accuracy_score(y_test, preds))\n",
    "    print(classification_report(y_test, preds))\n",
    "\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b10b9c5-75d4-469a-ad50-3e7fadf33f4f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
